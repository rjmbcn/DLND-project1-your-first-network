\documentclass[11pt,onecolumn]{article}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage[affil-it]{authblk}
\usepackage[margin=0.75in]{geometry}
\title{Neural networks}
\author{@rjmbcn14}
%\affil{}

\begin{document}
\maketitle 

\textbf{Neural networks, setup, architecture definitions.} Neural networks consists of $L$ layers labeled 
$l = 1,2 ...L$. In a given $l$ layer there are $n_l$ cells, whose activation outputs are denoted 
$a_j^{l}$ and given by 
\begin{equation}
	a_j^{l} = f(z^l_{j}),
\end{equation}
where $f(z^l_{j})$ is known as the activation function whose argument is given by  
\begin{equation}
	z^l_{j} = \sum_{k=1}^{n_{l-1}} w_{jk}a^{l-1}_k,
\end{equation}
here the weight $w^{l}_{jk}$ connects the output $a^{l-1}_k$ of the $k$-th cell in the previous layer 
to the $j$ cell in the $l$-th layer. 

Matrix and vector notation is used to efficiently represent a neural network. 
Considering layer $l$ its activation outputs are contained in a column vector 
$\mathbf{a}^{l} = [a^l_1 ~ a^l_2~...~a^l_{n_l}]^{T}$, given by 

\begin{eqnarray}
	\mathbf{a}^{l} &=& f\left( \mathbf{z}^{l} \right) 
\end{eqnarray}
where the input is
\begin{eqnarray}
	\mathbf{z}^{l}   &=& \mathbf{W}^{l} \mathbf{a}^{l-1},
\end{eqnarray} 
with the weight matrix 

\begin{equation}
	\mathbf{W}^{l} = \left(\begin{array}{cccc} w_{11}^{l} & w_{12}^{l} & \hdots & w_{1n_{l-1}}^{l} \\ 
	w_{21}^{l} & w_{22}^{l} &\hdots & w_{2{n_{l-1}}}^{l}  \\
	\vdots & \vdots & &\vdots \\
	w_{n_{l}1}^{l} & w_{n_{l}2}^{l} &\hdots & w_{n_{l}{n_{l-1}}}^{l} 
	\end{array}\right).
\end{equation}

\textit{Input and output layers.} The activation vector of the input layer consists of the input vector 
$\mathbf{x}$. For each input there is an observation vector $\mathbf{y}$. The activation vector of the 
output layer is denoted by $\mathbf{\hat{y}}$

\textit{Hidden layers.} Hidden layers are those layers connecting the input layer to the output layer.

\textit{Training a neural network.}  A network is trained by adjusting the elements in 
its weight matrices $\mathbf{W}^{l}$ to yield a minimum value in its cost function $\mathrm{C}$. Given 
a set of inputs and observations $\{\mathbf{x},\mathbf{y}\}$ the cost function can defined as 

\begin{equation}
    \mathrm{C} = \frac{1}{2}\sum_j(y_j - \hat{y}_j)^2
\end{equation}
other cost functions could be used. The typical training/optimization technique is gradient descent.  

\textbf{Network optmization.} Given a set of known inputs and known observations 
$\{\mathbf{x},\mathbf{y}\}$ a network is optimized by minimizing its cost function $\mathrm{C}$. 
This is typically achieved by first initializing the weights $\mathbf{W}^{l}$. 
Then given a subset (also known as a \textit{batch}) of the inputs 
$\mathbf{x}$ is used to compute the network outputs $\mathbf{\hat{y}}$. 
This step is known as forward propagation. From the computed network outputs and the known outputs 
the cost function is computed, for instance using the squared error 

\begin{equation}
    \mathrm{C} = \frac{1}{2}\sum_j(y_j - \hat{y}_j)^2
\end{equation}

The typical optimization method, i.e. finding minima in the Cost function, is gradient descent. 

\textbf{Gradient descent.} (From wikipedia) Gradient descent is a first-order iterative optimization 
algorithm. To find a local minimum of the cost function using gradient descent, 
one takes steps proportional to the negative of the gradient of the function with respect weights at 
the current point:

\begin{equation}
	\Delta \mathbf{W}^{l} = - \gamma \nabla_{\mathbf{W}^{l}} \mathrm{C},
\end{equation}
where proportionality factor $\gamma$ is a positive constant known as the learning rate of the network. 

\textbf{Backpropagation.} Backpropagation is the name that gradient descent receives when used to 
train a neural network. It consists in computing the error, for a given set of 
$\{\mathbf{x},\mathbf{y}\}$, and propagating it through the different hidden layers in the network so that 
$\Delta \mathbf{W}^{l}$ can be computed.

The following equations describe a summary of backpropagation, including matrix notation where $\odot$ and $\otimes$
denote element-wise matrix multiplication and outer-product respectively.

\begin{itemize}
  	\item Error in output layer: 
	\begin{eqnarray}
	        \delta^{L}_{j} &=& \frac{ \partial C}{\partial a^{L}_{j}}f^{'}\left( {z}^{L}_{j}\right) \rightarrow \mathrm{~matrix ~ form:} \\
		\delta^{L} &=& \nabla_{\mathbf{a}}C \odot g^{'}\left( \mathbf{h}^{L} \right) 
		% \delta^{L} &= &  'np.multiply( \rm{partial\_cost\_partial\_activation*h\_prime})' 
	\end{eqnarray} 
	\item Error in hidden layer: 
	\begin{eqnarray}
		\delta^{l-1}_{k} &=& \sum_{j}w^{l}_{jk}\delta^{l}_{j}f^{'}\left( h^{l-1}_k \right) \rightarrow : \\
		\delta^{l-1} &=& \left(\left(\mathbf{W}^{l}\right)^{T}\delta^{l} \right) \odot f^{'}\left( \mathbf{h}^{l-1}\right) 
	\end{eqnarray}
	\item Rate of change of cost function with respect bias in the network:  
	\begin{eqnarray}
		\frac{\partial C}{\partial b^{l}_{j} } &=& \delta^{l}_{j} \rightarrow : \\
		\frac{\partial C}{\partial \mathbf{b}^{l} } &=& \delta^{l}
	\end{eqnarray}
	\item Rate of change of cost function with respect weights in the network: 
	\begin{eqnarray}
	 	\frac{\partial C}{\partial w^{l}_{jk} } &=& \delta^{l}_{j} a^{l-1}_{k}\rightarrow : \\
		\frac{\partial C}{\partial \mathbf{W}} &=& \delta^{l} \otimes \mathbf{a}^{l-1} 
	\end{eqnarray} 
\end{itemize}

\textbf{Neural network performance.} A neural network is trained on a set of data 
$\{\mathbf{x},\mathbf{y}\}$ with the purpose to make predictions on the outcome of a 
process given a new set of input values. Two common pitfalls can limit the quality of predictions: 
oversimplication of network model and complexity of the model. Oversimplification of the model 
can produce \textit{underfitting} of the training data, that is a model that misses important aspects of 
the process in their predictions. On the other had a high level of complexity in the model 
can produce \textit{overfitting}, that is a model that is too specific for the given training data set. 
A healthy balance is achieved in the middle, that is a network model that takes into account the most 
relevant aspects of the process but that is general enough to make accurate predictions based on a 
new set of inputs. 

To achieve such a model the data available to train a network is divided into three mutually exclusive sets: 
\emph{training data}, \emph{validation data}, and \emph{testing data}. 
The approach consists on tuning the weights in the 
network using the \emph{training data}, then making predictions using the \emph{validation data}. 
\emph{Underfitting} of the model is shown when the cost function, normalized to a relevant scale,  evaluated on the validation set 
is too high. 
\emph{Overfitting} is present when the cost function for the training data is low but the cost computed based on 
the validation data differs significantly from the training data cost. 
When both costs agree with each other the network is said to be well trained, its perfromance can then be 
estimated by calculating the cost of the nework using the testing data set. 

\textbf{Advanced neural networks}
Some problems are better tackled using specialized versions of neural networks, such as in convolutional 
neural networks (CNN) and recurrent-neural networks (RNN). These two types of neural networks are 
implemented taking advantage of known features in the input data such as in images or in time series. 
The idea behind is simple: exploit known structures in the input data to make the learning and inference 
process of the network more efficient.

\textbf{Derivation of backpropagation equations} It is all about the chain rule. The goal is to find
the weights that minimize the cost function $C$ of the model. This optimization is implemented numerically 
using gradient descent 

\begin{equation}
	\mathbf{W}^{l} = \mathbf{W}^{l} - \gamma \nabla_{\mathbf{W}^{l}} \mathrm{C},
\end{equation}

To get expressions for the rate of change of the cost function with respect weights consider the following.
Starting with weights connecting cells in the second-to-last layer, indexed with $k$, to cells in the output 
layer (index $j$) we obtain 
\begin{equation}
    \frac{\partial C}{\partial w^{L-1}_{jk}} =  \frac{\partial C}{\partial a^{L}_j} 
                                               \frac{\partial a^{L}_j}{\partial z^{L}_j}
                                               \frac{\partial z^{L}_j}{\partial w_{jk} },
\end{equation}

where 
\begin{eqnarray}
    z^{L}_j &=& \sum_k w_{jk}^{L-1} a^{L-1}_k. 
\end{eqnarray}  
Thus 
\begin{equation}
    \frac{\partial C}{\partial w^{L-1}_{jk}} =  \frac{\partial C}{\partial a^{L}_j} 
                                               \frac{\partial a^{L}_j}{\partial z^{L}_j}
                                               a^{L-1}_k.
\end{equation}

Defining 
\begin{equation}
    \delta_j^{L} = \frac{\partial C}{\partial a^{L}_j} f'(z^{L}_j)
\end{equation}
we get 
\begin{equation}
    \frac{\partial C}{\partial w^{L-1}_{jk}} =  \delta_j^{L} a^{L-1}_k ,
\end{equation}    

which corresponds to the elements of the matrix 
\begin{equation}
    \frac{\partial C}{\partial \mathbf{W}^{L-1}} = \delta^{L} \otimes \mathbf{a}^{L-1}. 
\end{equation}

To get an expression for the rate of change of the cost function with respect to coefficients 
connecting cells in the third-to-last layer (indexed with $l$,) to cells
in the second-to-last layer (with index $k$) consider 

\begin{eqnarray}
    \frac{\partial C}{\partial w^{L-2}_{kl}} &=& \sum_j \frac{\partial C}{\partial a^{L}_j} 
                                               \frac{\partial a^{L}_j}{\partial z^{L}_j}
                                               \frac{\partial z^{L}_j}{\partial w_{kl}^{L-2} } \\
    \frac{\partial C}{\partial w^{L-2}_{kl}} &=& \sum_j \delta_j^{L}  \frac{\partial z^{L}_j}{\partial w_{kl}^{L-2} }                                         
\end{eqnarray}    


where the sum running over $j$ is due to the fact that the $ w^{L-2}_{kl} $ weight
in the $L-2$ layer couples to all cells in the $L$ layer by the output $a^{L-1}_k$ of the $k$ cell 
in the $L-1$ layer.   

Note that 
\begin{eqnarray}
    z^{L}_j &=& \sum_k w_{jk}f(z^{L-1}_k) \\ 
    z^{L}_j &=& \sum_k w_{jk}f(\sum_l w_{kl}^{L-2} a^{L-2}_l)
\end{eqnarray}    
thus, following the chain rule one obtains


\begin{eqnarray}
    \frac{\partial z^{L}_j}{\partial w_{kl}^{L-2} } &=& \sum_k w_{jk}^{L-1} \frac{\partial f(z^{L-1}_j)}{\partial  w_{kl}^{L-2}} \\
    \frac{\partial z^{L}_j}{\partial w_{kl}^{L-2} } &=& \sum_k w_{jk}^{L-1}  f'(z^{L-1}_k) \frac{\partial z^{L-1}_k}{\partial  w_{kl}^{L-2}} \\   
    \frac{\partial z^{L}_j}{\partial w_{kl}^{L-2} } &=& \sum_k w_{jk}^{L-1}  f'(z^{L-1}_k) a^{L-2}_l , \\
\end{eqnarray}

which can be cast in matrix form as 

\begin{equation}
    \frac{\partial C}{\partial \mathbf{W}^{L-2}} = \delta^{L-1} \otimes \mathbf{a}^{L-2}. 
\end{equation}
where
\begin{eqnarray}
    \delta^{L-1} &=& \left(\left(\mathbf{W}^{L-1}\right)^{T}\delta^{L} \right) \odot f^{'}\left( \mathbf{z}^{L-1}\right) 
\end{eqnarray}.

It can be proved that for any inner layer $M$

\begin{eqnarray}
    \delta^{M} &=& \left(\left(\mathbf{W}^{M}\right)^{T}\delta^{M+1} \right) \odot f^{'}\left( \mathbf{z}^{M}\right), 
\end{eqnarray}

enabling one to propagate the error in the output to all inner layers. 


\end{document} 